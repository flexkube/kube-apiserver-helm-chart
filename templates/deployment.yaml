apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-apiserver
  labels:
    k8s-app: kube-apiserver
spec:
  replicas: {{ .Values.replicas }}
  selector:
    matchLabels:
      k8s-app: kube-apiserver
  strategy:
    type: RollingUpdate
    rollingUpdate:
      # We listen on the hostPort, so when having only a one replica, maxUnavailable: 0
      # prevents updates from rolling as 2 processes cannot listen on a single port.
      maxUnavailable: {{ .Values.maxUnavailable }}
  template:
    metadata:
      labels:
        k8s-app: kube-apiserver
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
        checksum/secret: {{ include (print $.Template.BasePath "/secret.yaml") . | sha256sum }}
    spec:
      affinity:
        podAntiAffinity:
          {{- if eq .Values.replicas 1.0 }}
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: k8s-app
                  operator: In
                  values:
                  - kube-apiserver
              topologyKey: kubernetes.io/hostname
          {{- else }}
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-apiserver
            topologyKey: kubernetes.io/hostname
          {{- end }}
      # Host network is required to be able to communicate with etcd, which runs outside of the cluster.
      # When running single control plane node, this is not really needed, but it simplifies the config
      # and makes updating slightly better, as the alternative is to use hostPort, which prevents pod from being
      # scheduled if previous pod still occupies the node. This make update rely on kube-scheduler without kube-apiserver
      # running.
      # With `hostNetwork:true` pod is scheduled on the node, but it will be in crash loop until previous one is manually
      # removed, this is OK.
      hostNetwork: true
      nodeSelector:
        node-role.kubernetes.io/master: ""
      priorityClassName: system-cluster-critical
      serviceAccountName: kube-apiserver
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: kube-apiserver
        image: {{ .Values.image | default (printf "k8s.gcr.io/hyperkube:v%s" .Chart.AppVersion) }}
        command:
        - /hyperkube
        - kube-apiserver
        - --etcd-servers={{ .Values.etcdServers }}
        {{- if .Values.bindOnHostIP }}
        # We run on host network, so we bind only on host IP, which ideally would not be a public interface.
        - --bind-address=$(HOST_IP)
        {{- else }}
        - --bind-address={{ .Values.bindAddress }}
        {{- end }}
        # We run on host network, but we want to advertise Pod IP address to kubernetes.default.svc service endpoint.
        # Without this line, we may get public IP address being added to the endpoint, which is not correct.
        - --advertise-address=$(HOST_IP)
        # CA certificate for validating API clients.
        - --client-ca-file=/etc/kubernetes/pki/ca.crt
        # TLS certificates for HTTPS serving.
        - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
        - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
        # Required for TLS bootstrapping.
        - --enable-bootstrap-token-auth=true
        # Override default service cluster IP, as it conflicts with host CIDR.
        - --service-cluster-ip-range={{ .Values.serviceCIDR }}
        # To disable access without authentication.
        - --insecure-port=0
        # Since we will run self-hosted K8s, pods like kube-proxy must run as privileged containers, so we must allow them.
        - --allow-privileged=true
        - --authorization-mode=RBAC,Node
        # Required to validate service account tokens created by controller manager.
        - --service-account-key-file=/etc/kubernetes/pki/service-account.crt
        # Allow to customize TLS port, as some people may want to run it on custom port,
        # for security, to have a LB in front of it or something.
        - --secure-port={{ .Values.securePort }}
        # Prefer to talk to kubelets over InternalIP rather than via Hostname or DNS, to make it more robust.
        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
        # Required for enabling aggregation layer.
        - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
        - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
        - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
        - --requestheader-allowed-names=
        - --requestheader-extra-headers-prefix=X-Remote-Extra-
        - --requestheader-group-headers=X-Remote-Group
        - --requestheader-username-headers=X-Remote-User
        # Required for kubelet communication.
        - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
        - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
        # Secure communication to etcd servers.
        - --etcd-cafile=/etc/kubernetes/pki/etcd-ca.crt
        - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
        - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
        - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy
        livenessProbe:
          httpGet:
            scheme: HTTPS
            path: /livez
            port: {{ .Values.securePort }}
          initialDelaySeconds: 30
          timeoutSeconds: 30
        env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        volumeMounts:
        - name: secrets
          mountPath: /etc/kubernetes/pki
          readOnly: true
      volumes:
      - name: secrets
        secret:
          secretName: kube-apiserver
