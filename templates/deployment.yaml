apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-apiserver
  labels:
    k8s-app: kube-apiserver
spec:
  replicas: {{ .Values.replicas }}
  selector:
    matchLabels:
      k8s-app: kube-apiserver
      component: apiserver
      provider: kubernetes
  strategy:
    type: RollingUpdate
    rollingUpdate:
      # If there is just one replica, overcommit and temporarily run 2 replicas to avoid downtime.
      {{- if eq (int .Values.replicas) 1 }}
      maxUnavailable: 0
      {{- else }}
      maxUnavailable: 1
      {{- end }}
  template:
    metadata:
      labels:
        k8s-app: kube-apiserver
        component: apiserver
        provider: kubernetes
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
        # If any of the secret changes, make rolling restart to apply the change.
        checksum/secret: {{ include (print $.Template.BasePath "/secret.yaml") . | sha256sum }}
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            k8s-app: kube-apiserver
      # If there is more than one replica needed, make sure they run on different hosts to avoid
      # RAM starvation on smaller instances.
      {{- if gt (int .Values.replicas) 1 }}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-apiserver
            topologyKey: kubernetes.io/hostname
      {{- end }}
      # With self-hosted kube-apiserver, running on Pod CIDR is difficult, as then kube-apiserver availability relies on kube-proxy (handles translative Service IPs to Pod IPs), CNI (handles assigning IP addresses to Pods) etc., which makes it less robust and we need to absolutely make sure that kube-apiserver is available all the time.
      hostNetwork: true
      nodeSelector:
        # This node label is a privileged label, which means, that kubelet is not able to set it for itself, which prevents rogue cluster nodes to attract kube-apiserver pod, which would result in leaking all cluster secrets.
        node-role.kubernetes.io/master: ""
      priorityClassName: system-cluster-critical
      serviceAccountName: kube-apiserver
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: kube-apiserver
        image: {{ .Values.image | default (printf "k8s.gcr.io/kube-apiserver:v%s" .Chart.AppVersion) }}
        command:
        - kube-apiserver
        - --etcd-servers={{ required "etcdServers can't be empty!" (join "," .Values.etcdServers) }}
        - --bind-address=$(HOST_IP)
        # We run on host network, but we want to advertise Pod IP address to kubernetes.default.svc service endpoint.
        # Without this line, we may get public IP address being added to the endpoint, which is not correct.
        - --advertise-address=$(HOST_IP)
        # CA certificate for validating API clients.
        - --client-ca-file=/etc/kubernetes/pki/ca.crt
        # TLS certificates for HTTPS serving.
        - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
        - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
        # Required for TLS bootstrapping.
        - --enable-bootstrap-token-auth=true
        # Override default service cluster IP, as it conflicts with host CIDR.
        - --service-cluster-ip-range={{ .Values.serviceCIDR }}
        # Since we will run self-hosted K8s, pods like kube-proxy must run as privileged containers, so we must allow them.
        - --allow-privileged=true
        - --authorization-mode=RBAC,Node
        # Required to validate service account tokens created by controller manager.
        - --service-account-key-file=/etc/kubernetes/pki/service-account.key
        # Allow to customize TLS port, as some people may want to run it on custom port,
        # for security, to have a LB in front of it or something.
        - --secure-port={{ .Values.securePort }}
        # Prefer to talk to kubelets over InternalIP rather than via Hostname or DNS, to make it more robust.
        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
        # Required for enabling aggregation layer.
        - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
        - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
        - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
        - --requestheader-allowed-names=
        - --requestheader-extra-headers-prefix=X-Remote-Extra-
        - --requestheader-group-headers=X-Remote-Group
        - --requestheader-username-headers=X-Remote-User
        # Required for kubelet communication.
        - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
        - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
        - --kubelet-certificate-authority=/etc/kubernetes/pki/ca.crt
        # Secure communication to etcd servers.
        - --etcd-cafile=/etc/kubernetes/pki/etcd-ca.crt
        - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
        - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
        # Enable additional admission plugins:
        # - NodeRestriction for extra protection against rogue cluster nodes.
        # - PodSecurityPolicy for PSP support.
        - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy
        # Use SO_REUSEPORT, so multiple instances can run on the same controller for smooth upgrades.
        - --permit-port-sharing=true
        # To taint node after 60 seconds instead of 300 to allow faster failover.
        - --default-not-ready-toleration-seconds=60
        - --default-unreachable-toleration-seconds=60
        # New flags required for TokenRequest feature.
        - --service-account-issuer=https://kubernetes.default.svc
        - --service-account-signing-key-file=/etc/kubernetes/pki/service-account.key
        env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        volumeMounts:
        - name: secrets
          mountPath: /etc/kubernetes/pki
          readOnly: true
        resources:
          requests:
            cpu: 300m
            memory: 1Gi
          limits:
            memory: 3Gi
      volumes:
      - name: secrets
        secret:
          secretName: kube-apiserver
